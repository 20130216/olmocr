{"id": "486098ff06e77c27bbfcc3ae63598fc706cb6cbd", "text": "dolma: an Open Corpus of Three Trillion Tokens\nfor Language Model Pretraining Research\n\nLuca Soldaini     Rodney Kinney     Akshita Bhagia     Dustin Schwenk\nDavid Atkinson    Russell Authur    Ben Bogin  \u03c9      Khyathi Chandu\nJennifer Dumas    Yanai Elazar  \u03c9   Valentin Hofmann  Ananya Harsh Jha\nSachin Kumar      Li Lucy \u03b2         Xinyi Lyu \u03c9       Nathan Lambert    Ian Magnusson\nJacob Morrison    Niklas Muennighoff  Aakanksha Naik  Crystal Nam\nMatthew E. Peters \u03c4  Abhilasha Ravichander  Kyle Richardson  Zejiang Shen \u03c4\nEmma Strubell \u03c7   Nishant Subramani \u03c7  Oyvind Tafjord  Pete Walsh\nLuke Zettlemoyer \u03c9  Noah A. Smith \u03c9  Hannaneh Hajishirzi \u03c9\nIz Beltagy        Dirk Groeneveld   Jesse Dodge\n\nKyle Lo\n\nAllen Institute for AI      \u03b2 University of California, Berkeley      \u03c7 Carnegie Mellon University\n\u03c4 Spiffy AI      \u03c4 Massachusetts Institute of Technology      \u03c9 University of Washington\n{lucas,kylel}@allenai.org\n\nAbstract\nInformation about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.\u00b9\n\nhf.co/datasets/allenai/dolma\ngithub.com/allenai/dolma\n\n\u00b9 Core authors. See Appendix B for list of contributions.\n\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15725\u201315778\nAugust 11-16, 2024 \u00a92024 Association for Computational Linguistics\n\n1 Introduction\nLanguage models are now central to tasks including summarization, question answering, and more. Increasingly, the most powerful language models are built by a few organizations who withhold model development details (Anthropic, 2023a; OpenAI, 2023; Gemini Team et al., 2023). In particular, the composition of language model pretraining data is often vaguely described, even in cases where the model itself is released for public use, such as Llama 2 (Touvron et al., 2023b). This hinders understanding of the effects of pretraining corpus composition on model capabilities and limitations, with impacts on scientific progress as well as on the public who interfaces with these models. Our aim is to increase participation in scientific research of language models through open corpora:\n\n\u2022 Data transparency helps developers and users of applications that rely on language models to make more informed decisions (Gebru et al., 2021). For example, models have shown to perform better on tasks that are more similar to their pretraining data (Razeghi et al., 2022; Kandpal et al., 2023), or social biases in models\u2019 pretraining data may necessitate additional consideration when using them (Feng et al., 2023; Navigli et al., 2023; Seshadri et al., 2023).\n\n\u2022 Open pretraining data is necessary to analyze how\n\n\u00b9 This manuscript was prepared for Dolma v.1.6. As our work on open data for language modeling continues, we will continue to improve Dolma. Updated versions can be found in the provided links.", "source": "olmocr", "added": "2025-05-23", "created": "2025-05-23", "metadata": {"Source-File": "/Users/wingzheng/Desktop/github/ParseDoc/olmocr/tests/gnarly_pdfs/dolma-page-1.pdf", "olmocr-version": "0.1.69", "pdf-total-pages": 1, "total-input-tokens": 4526, "total-output-tokens": 866, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 3902, 1]]}}
